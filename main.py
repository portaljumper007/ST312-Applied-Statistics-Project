import pandas as pd
import numpy as np
from collections import defaultdict
from scipy.signal import stft
from scipy.ndimage import gaussian_filter
import os
import pickle
import pandas as pd
import plotly.graph_objs as go
from plotly.subplots import make_subplots
import plotly.express as px
import statsmodels.api as sm
from scipy.stats import norm
import scipy

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from statsmodels.tsa.seasonal import STL
from scipy.signal import correlate
from sklearn.preprocessing import StandardScaler
from scipy.stats import pearsonr, spearmanr

import charts_NLP_v2

def load_or_create_topic_strengths(filename, num_topics):
    if os.path.exists('topic_info.pkl'):
        with open(filename, 'rb') as file:
            return pickle.load(file)
    else:
        weekly_topic_strengths_data = charts_NLP_v2.main(show_graph=True, num_topics=num_topics) # Generate data
        data_to_save = {week: {topic: strength for topic, strength in topics.items()} # Extract and save only the necessary data from weekly_topic_strengths
                        for week, topics in weekly_topic_strengths_data.items()}
        with open(filename, 'wb') as file:
            pickle.dump(data_to_save, file)
        return weekly_topic_strengths_data

def get_week_year(date):
    return f"{date.isocalendar()[0]}-W{date.isocalendar()[1]}"

def main():
    num_topics = 10
    weekly_topic_strengths = load_or_create_topic_strengths('weekly_topic_strengths_norm.pkl', num_topics)




    # INFO ABOUT TOPICS (eg constitutent words and their strengths)
    with open('topic_info.pkl', 'rb') as file:
        topic_info = pickle.load(file) # Load topic information

    print("Topic Modelling Results:")
    print("The following table presents the top 10 words and their associated probabilities for each topic generated by the LDA model:")
    print("Topic\tTop Words and Probabilities")
    for topic, top_words in topic_info.items():
        topic_words = ", ".join([f"({word}, {prob:.4f})" for word, prob in top_words])
        print(f"{topic+1}\t{topic_words}")
    print("These topics provide a high-level overview of the main themes and concepts present in the music chart data, and serve as the basis for our analysis of the relationship between weather patterns and music trends.")

    topic_words_fig = make_subplots(rows=num_topics, cols=1, subplot_titles=[f"Topic {i+1}" for i in range(num_topics)]) # Create a new figure for displaying top words for each topic
    for topic in range(num_topics):
        top_words = topic_info[topic]
        words, probs = zip(*top_words)
        topic_words_fig.add_trace(go.Bar(x=probs, y=words, orientation='h', name=f"Topic {topic+1}"), row=topic+1, col=1)
        topic_words_fig.update_xaxes(title_text="Probability", row=topic+1, col=1)
        topic_words_fig.update_yaxes(title_text="Words", row=topic+1, col=1)
    topic_words_fig.update_layout(title=f"Top Words for Each Topic",
                                height=200*num_topics, width=800,
                                showlegend=False)
    #topic_words_fig.show() #FIGURE SHOW

    #TOPIC STRENGTH MATRIX
    import plotly.figure_factory as ff
    z = []
    annotations = []
    for topic, top_words in topic_info.items():
        words, probs = zip(*top_words)
        z.append(list(probs))
        annotations.append([f"{word}<br>{prob:.4f}" for word, prob in zip(words, probs)])
    x = list(range(10))  # x-axis labels (1st, 2nd, ..., 10th)
    y = [f"Topic {i+1}" for i in range(num_topics)]  # y-axis labels (Topic 1, Topic 2, ...)
    # Create the annotated heatmap
    word_strength_matrix = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=annotations, colorscale='blues', zmid=0)
    word_strength_matrix.update_layout(title='Top Words and Strengths for Each Topic',height=1200,width=1200,xaxis=dict(tickvals=np.arange(10), ticktext=['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th']),yaxis=dict(tickvals=[i for i in range(num_topics)], ticktext=[f"Topic {i+1}" for i in range(num_topics)]),showlegend=False)
    #word_strength_matrix.show() #FIGURE SHOW




    dfs = []
    mse_scores_all, mae_scores_all, r2_scores_all = [], [], []
    for location in ["Chicago","LA"]:
        print("@"*(40+len(location)))
        print("-"*20+str(location)+"-"*20)
        print("@"*(40+len(location)))

        # Load and preprocess weather data
        weather_df = pd.read_csv(location+" Weather.csv")
        if location == "LA":
            weather_df['DATE'] = pd.to_datetime(weather_df['DATE'], format='%d/%m/%Y') #LA
        elif location == "Chicago":
            weather_df['DATE'] = pd.to_datetime(weather_df['DATE'], format='%Y-%m-%d') #CHICAGO
        weather_df.set_index('DATE', inplace=True)

        # Select only numeric columns relevant to the analysis
        numeric_columns = ['PRCP', 'SNOW', 'TMAX', 'TMIN']
        weather_numeric = weather_df[numeric_columns]
        weekly_weather = weather_numeric.groupby(pd.Grouper(freq='W')).mean()

        weekly_weather_for_combination = weekly_weather.copy(deep=True)
        dfs.append(weekly_weather_for_combination)
        weekly_weather_for_autocorr = weekly_weather.copy()
        weekly_weather_for_autocorr.fillna(0, inplace=True)

        # Align weekly_topic_strengths keys to week-year format
        normalized_topic_strengths = {get_week_year(pd.Timestamp(week)): strengths
                                    for week, strengths in weekly_topic_strengths.items()}

        # Aligning topic strengths with weather data
        topic_strengths_aligned = defaultdict(list)
        for week in weekly_weather.index:
            week_year = get_week_year(week)
            for topic in range(num_topics):
                if week_year in normalized_topic_strengths:
                    strength = normalized_topic_strengths[week_year].get(topic, 0)
                else:
                    strength = 0
                topic_strengths_aligned[topic].append(strength)

        # STFT analysis and autocorrelation
        window_size = 52*10  # Approx a year
        autocorrelation_results = defaultdict(list)
        for topic, strengths in topic_strengths_aligned.items():
            strengths_filled = np.nan_to_num(strengths) # Filling NaNs with zeros
            f, t, Zxx = stft(strengths_filled, window='hamming', nperseg=window_size)
            #print(f"STFT Output Shape for Topic {topic}: {Zxx.shape}, Time Length: {len(t)}")
            autocorr = [np.correlate(Zxx[:,i], weekly_weather_for_autocorr['TMAX'], mode='same') for i in range(len(t))]
            autocorr_sum = [np.sum(np.abs(a)) for a in autocorr]
            autocorrelation_results[topic] = autocorr_sum
            #print(f"Autocorrelation data for topic {topic}:", autocorr_sum)
        
        #Correlation Analysis between Weather Variables and Topic Strengths
        results = []
        for topic in range(num_topics):
            topic_strengths = topic_strengths_aligned[topic]
            for feature in numeric_columns:
                weather_values = weekly_weather[feature].values
                strengths_filled = np.nan_to_num(topic_strengths)
                weather_filled = np.nan_to_num(weather_values)
                correlation, p_value = scipy.stats.pearsonr(strengths_filled, weather_filled)
                results.append([topic+1, feature, correlation, p_value])
        print("\nCorrelation Analysis Results:")
        columns = ["Topic", "Weather Variable", "Correlation", "p-value"]
        corr_df = pd.DataFrame(results, columns=columns)
        print(corr_df.to_string(index=False))

        weekly_topic_strengths_aggregated = defaultdict(lambda: np.zeros(53))
        for date, strengths in weekly_topic_strengths.items():
            week_of_year = pd.Timestamp(date).isocalendar()[1]
            for topic in range(num_topics):
                weekly_topic_strengths_aggregated[topic][week_of_year - 1] += strengths.get(topic, 0)

        fig = make_subplots(rows=3, cols=4, specs=[[{}, {}, {}, {}], [{}, {}, {}, {}], [{}, {}, {}, {}]],
                            subplot_titles=("Topic Strength Over Time", "Autocorrelation of Topic Strengths with Max Temperature", "Smoothed NLP Strengths by Week of the Year", "NLP Strengths by Week of the Year", "Precipitation vs Topic Strength", "Max Temperature vs Topic Strength", "Snow vs Topic Strength", "Min Temperature vs Topic Strength", "LOESS: Precipitation vs Topic Strength", "LOESS: Max Temperature vs Topic Strength", "LOESS: Snow vs Topic Strength", "LOESS: Min Temperature vs Topic Strength"))

        # Autocorrelation of Topic Strengths with Max Temperature on the first row
        for topic, autocorr in autocorrelation_results.items():
            fig.add_trace(go.Scattergl(x=weekly_weather.index, y=autocorr, mode='lines', name=f'Topic {topic + 1} Autocorrelation'), row=1, col=2)

        # Initialize a dictionary to hold time series data for each topic
        topic_time_series = {topic: [] for topic in range(num_topics)}
        dates = sorted(weekly_topic_strengths.keys())
        # Aggregate data for each topic
        for date in dates:
            for topic in range(num_topics):
                strength = weekly_topic_strengths[date].get(topic, 0)
                topic_time_series[topic].append((date, strength))
        # Plot each topic's time series data
        for topic, data in topic_time_series.items():
            dates, strengths = zip(*data)  # Unpack the tuples into two lists
            fig.add_trace(go.Scattergl(x=dates, y=strengths, mode='lines', name=f'Topic {topic} Strength Over Time'), row=1, col=1)

        # Ensure "NLP Strengths by Week of the Year" plots aggregated data correctly
        for topic, strengths in weekly_topic_strengths_aggregated.items():
            fig.add_trace(go.Scattergl(x=np.arange(1, 54), y=strengths, mode='lines', name=f'Topic {topic + 1} NLP Strengths by Week'), row=1, col=4)

        # Smoothed NLP Strengths by Week of the Year, third graph on the first row
        for topic, strengths in weekly_topic_strengths_aggregated.items():
            smoothed_strengths = gaussian_filter(strengths, sigma=2)
            fig.add_trace(go.Scattergl(x=np.arange(1, 54), y=smoothed_strengths, mode='lines', name=f'Topic {topic + 1} Smoothed'), row=1, col=3)

        # Scatter graphs and fitted lines, maintaining the color consistency
        topic_colors = px.colors.qualitative.Plotly
        for i, feature in enumerate(['PRCP', 'TMAX', 'SNOW', 'TMIN']):
            for topic in range(num_topics):
                color = topic_colors[topic % len(px.colors.qualitative.Plotly)]
                x = weekly_weather[feature].values
                y = np.array(topic_strengths_aligned[topic])
                fig.add_trace(go.Scattergl(x=x, y=y, mode='markers', marker=dict(color=color, opacity=0.25), name=f'Topic {topic+1} vs {feature}'), row=2, col=i+1)
                # Filter out NaN values which can cause errors in LOESS
                valid_indices = ~np.isnan(x) & ~np.isnan(y)
                x_filtered, y_filtered = x[valid_indices], y[valid_indices]
                # Apply LOESS smoothing
                lowess = sm.nonparametric.lowess(y_filtered, x_filtered, frac=0.25)
                # lowess result is a two-column array, first column is x, second column is y
                x_lowess, y_lowess = lowess[:, 0], lowess[:, 1]
                fig.add_trace(go.Scattergl(x=x_lowess, y=y_lowess, mode='lines', line=dict(color=color), name=f'LOESS Fit Topic {topic+1}'), row=3, col=i+1)

        # Update axes and layout
        fig.update_xaxes(title_text="Date", row=1, col=1)
        fig.update_xaxes(title_text="Week of the Year", row=1, col=3)
        fig.update_xaxes(title_text="Precipitation (in)", row=2, col=1)
        fig.update_xaxes(title_text="Max Temperature (°F)", row=2, col=2)
        fig.update_xaxes(title_text="Snow (in)", row=2, col=3)
        fig.update_xaxes(title_text="Min Temperature (°F)", row=2, col=4)
        fig.update_layout(height=2560, title_text=f"Exploratory Analysis of Weather Trends and NLP of Music Charts - {location}",
                  legend=dict(x=1.02, y=1, borderwidth=1, bgcolor='rgba(255, 255, 255, 0.8)'),
                  hovermode='x unified')
        for i in range(1, 4):
            for j in range(1, 5):
                fig.update_yaxes(title_text="Topic Strength", row=i, col=j)
        #fig.show() #FIGURE SHOW




        
        weather_features = weekly_weather.copy().dropna()

        # Ensure topic_strengths is aligned with weather_features
        aligned_topic_strengths = {}
        for topic in range(num_topics):
            topic_strengths = np.array(topic_strengths_aligned[topic])
            # Ensure only to include lengths that match the weather_features after dropping NA
            aligned_topic_strengths[topic] = topic_strengths[:len(weather_features)]
        

        # Prepare a new Plotly figure with 2 columns for linear regression and Decision Tree results
        subplot_titles = []
        for i in range(num_topics):
            subplot_titles.append(f'Topic {i+1} Linear Regression')
            subplot_titles.append(f'Topic {i+1} Decision Tree')
        regression_fig = make_subplots(rows=num_topics, cols=2, subplot_titles=subplot_titles,
                                        horizontal_spacing=0.02, vertical_spacing=0.05)

        for topic in range(num_topics):
            topic_strengths = aligned_topic_strengths[topic]
            
            # Generate interaction terms
            weather_features_interaction = weather_features.copy()
            for col1 in weather_features_interaction.columns:
                for col2 in weather_features_interaction.columns:
                    if col1 != col2:
                        weather_features_interaction[f'{col1}*{col2}'] = weather_features_interaction[col1] * weather_features_interaction[col2]

            X = weather_features_interaction
            y = topic_strengths

            # Split data into training and testing sets
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

            # Linear Regression Model
            lr_model = LinearRegression()
            lr_model.fit(X_train, y_train)
            y_pred_lr = lr_model.predict(X_test)

            # Decision Tree Regressor
            tree_model = DecisionTreeRegressor(max_depth=5, min_samples_split=5, random_state=42)
            tree_model.fit(X_train, y_train)
            y_pred_tree = tree_model.predict(X_test)

            # Linear Regression Predictions Plot
            regression_fig.add_trace(go.Scatter(x=y_test, y=y_pred_lr, mode='markers', name='LR Predictions', marker=dict(color='LightSkyBlue', opacity=0.6)), row=topic+1, col=1)
            regression_fig.update_xaxes(title_text="Actual Topic Strength", row=topic+1, col=1)
            regression_fig.update_yaxes(title_text="Predicted Topic Strength", row=topic+1, col=1)

            # Decision Tree Predictions Plot
            regression_fig.add_trace(go.Scatter(x=y_test, y=y_pred_tree, mode='markers', name='Tree Predictions', marker=dict(color='MediumPurple', opacity=0.6)), row=topic+1, col=2)
            regression_fig.update_xaxes(title_text="Actual Topic Strength", row=topic+1, col=2)
            regression_fig.update_yaxes(title_text="Predicted Topic Strength", row=topic+1, col=2)

            regression_fig.add_trace(go.Scatter(x=np.unique(y_test), y=np.poly1d(np.polyfit(y_test, y_pred_lr, 1))(np.unique(y_test)), mode='lines', name='LR Best Fit', line=dict(color='Red')), row=topic+1, col=1)
            regression_fig.add_trace(go.Scatter(x=np.unique(y_test), y=np.poly1d(np.polyfit(y_test, y_pred_tree, 1))(np.unique(y_test)), mode='lines', name='Tree Best Fit', line=dict(color='Red')), row=topic+1, col=2)

        regression_fig.update_layout(height=5000, width=2560, title_text=f"Topic Strengths vs Weather Features: Linear Regression & Decision Tree Predictions - {location}")
        #regression_fig.show() #FIGURE SHOW






        scaler = StandardScaler()
        standardized_weather = pd.DataFrame(scaler.fit_transform(weekly_weather), columns=weekly_weather.columns, index=weekly_weather.index)

        # Time series decomposition and cross-correlation analysis
        subplot_titles = []
        for i in range(num_topics):
            subplot_titles.extend([f"Topic {i+1} Decomposition", f"Topic {i+1} Cross-Correlation"])
        decomposition_and_cross_fig = make_subplots(rows=num_topics, cols=2, subplot_titles=subplot_titles, vertical_spacing=0.02, horizontal_spacing=0.06)

        def hex_to_rgb(hex_color):
            hex_color = hex_color.lstrip('#')
            return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))

        for topic in range(num_topics):
            topic_strengths = aligned_topic_strengths[topic]
            
            # Time series decomposition using STL
            stl_result = STL(topic_strengths, period=52).fit()
            trend, seasonal, residual = stl_result.trend, stl_result.seasonal, stl_result.resid
            
            decomposition_and_cross_fig.add_trace(go.Scatter(x=weekly_weather.index, y=topic_strengths, mode='lines', name=f'Topic {topic+1} Strength'), row=topic+1, col=1)
            decomposition_and_cross_fig.add_trace(go.Scatter(x=weekly_weather.index, y=trend, mode='lines', name=f'Trend', line=dict(color='red')), row=topic+1, col=1)
            decomposition_and_cross_fig.add_trace(go.Scatter(x=weekly_weather.index, y=seasonal, mode='lines', name=f'Seasonal', line=dict(color='green')), row=topic+1, col=1)
            decomposition_and_cross_fig.add_trace(go.Scatter(x=weekly_weather.index, y=residual, mode='lines', name=f'Residual', line=dict(color='blue')), row=topic+1, col=1)
            
            decomposition_and_cross_fig.update_xaxes(title_text="Date", row=topic+1, col=1)
            decomposition_and_cross_fig.update_yaxes(title_text="Topic Strength", row=topic+1, col=1)
            
            # Cross-correlation analysis
            max_lag = 26  # Half a year
            for feature in ['PRCP', 'TMAX', 'SNOW', 'TMIN']:
                weather_feature = np.nan_to_num(standardized_weather[feature].values)
                cross_corr = correlate(topic_strengths, weather_feature, mode='same')
                lags = np.arange(-max_lag, max_lag + 1)
                cross_corr = cross_corr[len(cross_corr) // 2 - max_lag : len(cross_corr) // 2 + max_lag + 1]

                n_bootstraps = 150
                bootstrapped_cross_corr = np.zeros((n_bootstraps, len(lags)))
                for i in range(n_bootstraps):
                    idx = np.random.choice(len(topic_strengths), size=int(len(topic_strengths)*0.9), replace=False)
                    bootstrapped_topic_strengths = topic_strengths.copy()
                    bootstrapped_topic_strengths[idx] = np.mean(weather_feature)
                    bootstrapped_weather_feature = weather_feature.copy()
                    bootstrapped_weather_feature[idx] = np.mean(weather_feature)
                    bootstrapped_cross_corr[i] = correlate(bootstrapped_topic_strengths, bootstrapped_weather_feature, mode='same')[len(cross_corr) // 2 - max_lag : len(cross_corr) // 2 + max_lag + 1]

                ci_lower = np.percentile(bootstrapped_cross_corr, 2.5, axis=0)
                ci_upper = np.percentile(bootstrapped_cross_corr, 97.5, axis=0)

                line_color = px.colors.qualitative.Plotly[len(decomposition_and_cross_fig.data) % len(px.colors.qualitative.Plotly)]

                decomposition_and_cross_fig.add_trace(go.Scatter(x=lags, y=cross_corr, mode='lines', name=f'{feature} Cross-Correlation', line=dict(color=line_color), hovertemplate=f'{feature} Cross-Correlation<br>Lag: %{{x}} weeks<br>Correlation: %{{y:.2f}}'), row=topic+1, col=2)

                decomposition_and_cross_fig.add_trace(go.Scatter(
                    x=lags,
                    y=ci_upper,
                    mode='lines',
                    line=dict(width=0.75, color=f'rgba{(*hex_to_rgb(line_color), 0.5)}'),
                    showlegend=False,
                    hovertemplate=f'{feature} 95% Confidence Interval<br>Lag: %{{x}} weeks<br>Upper Bound: %{{y:.2f}}'
                ), row=topic+1, col=2)

                decomposition_and_cross_fig.add_trace(go.Scatter(
                    x=lags,
                    y=ci_lower,
                    mode='lines',
                    line=dict(width=0.75, color=f'rgba{(*hex_to_rgb(line_color), 0.5)}'),
                    fillcolor=f'rgba{(*hex_to_rgb(line_color), 0.025)}',
                    fill='tonexty',
                    showlegend=False,
                    hovertemplate=f'{feature} 95% Confidence Interval<br>Lag: %{{x}} weeks<br>Lower Bound: %{{y:.2f}}'
                ), row=topic+1, col=2)
            
            decomposition_and_cross_fig.update_xaxes(title_text="Lag (Weeks)", row=topic+1, col=2)
            decomposition_and_cross_fig.update_yaxes(title_text="Correlation Coefficient", row=topic+1, col=2)

        decomposition_and_cross_fig.update_layout(height=5000, width=2560, title_text=f"Time Series Decomposition and Cross-Correlation Analysis - {location}")
        #decomposition_and_cross_fig.show() #FIGURE SHOW





    # REAL WORLD FORECASTING SIMULATION
    from sklearn.ensemble import GradientBoostingRegressor
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    from itertools import chain

    def forecast_topic_strengths(topic_strengths, weather_data, lag_weeks=4):
        X, y = [], []
        for i in range(lag_weeks, len(topic_strengths)):
            topic_lags = topic_strengths[i-lag_weeks:i]
            weather_lags = weather_data.iloc[i-lag_weeks:i].values.flatten()
            X.append(np.concatenate((topic_lags, weather_lags)))
            y.append(topic_strengths[i])
        return np.array(X), np.array(y)

    lag_weeks = 16
    subplot_titles = [[f"Topic {i+1} - Actual vs Predicted", f"Topic {i+1} - Performance Metrics", f"Topic {i+1} - Weather Partial Dependence", f"Topic {i+1} - Actual vs Predicted (Weather Only)", f"Topic {i+1} - Performance Metrics (Weather Only)"] for i in range(num_topics)]
    mse_scores_all, mae_scores_all, r2_scores_all, pcc_scores_all, srcc_scores_all = [], [], [], [], []

    # Combine weather data from Chicago and LA
    combined_weather = pd.concat([df.add_prefix(f"{location}_") for location, df in zip(["Chicago", "LA"], dfs)], axis=1, join='inner')

    # Align topic strengths with combined weather data
    combined_topic_strengths_aligned = defaultdict(list)
    for week in combined_weather.index:
        week_year = get_week_year(week)
        for topic in range(num_topics):
            if week_year in normalized_topic_strengths:
                strength = normalized_topic_strengths[week_year].get(topic, 0)
            else:
                strength = 0
            combined_topic_strengths_aligned[topic].append(strength)

    weather_vars = ['PRCP', 'TMAX', 'SNOW', 'TMIN']

    for model_data in [("Chicago", dfs[0]), ("LA", dfs[1]), ("Combined", combined_weather)]:
        location, weather_data = model_data
        forecasting_fig = make_subplots(rows=num_topics, cols=5, subplot_titles=list(chain.from_iterable(subplot_titles)))
        mse_scores, mae_scores, r2_scores, pcc_scores, srcc_scores = [], [], [], [], []

        if location in ["Chicago", "LA"]:
            numeric_columns = ['PRCP', 'SNOW', 'TMAX', 'TMIN'] # Select only numeric columns relevant to the analysis
        else:
            numeric_columns = ['Chicago_PRCP', 'Chicago_SNOW', 'Chicago_TMAX', 'Chicago_TMIN', 'LA_PRCP', 'LA_SNOW', 'LA_TMAX', 'LA_TMIN'] # Select only numeric columns relevant to the analysis
        weather_numeric = weather_data[numeric_columns]
        weekly_weather = weather_numeric.groupby(pd.Grouper(freq='W')).mean()

        # Create the figure for Partial Dependence Plots
        pdp_fig = make_subplots(rows=num_topics, cols=4, subplot_titles=[f"Topic {i+1} {var}" for i in range(num_topics) for var in weather_vars])

        for topic in range(num_topics):
            print(f"Processing Topic {topic+1}/{num_topics} - {location}")
            topic_strengths = topic_strengths_aligned[topic] if location in ["Chicago", "LA"] else combined_topic_strengths_aligned[topic]
            X, y = forecast_topic_strengths(topic_strengths, weekly_weather, lag_weeks)

            X = np.where(np.isnan(X), np.ma.array(X, mask=np.isnan(X)).mean(axis=0), X) # Replace missing values with the mean of the corresponding feature

            # Define the training and testing indices
            train_size = int(0.8 * len(X))
            X_train, y_train = X[:train_size], y[:train_size]
            X_test, y_test = X[train_size:], y[train_size:]

            # Train the Gradient Boosting model
            gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.06, max_depth=4, subsample=0.8, random_state=42)
            gb_model.fit(X_train, y_train)
            y_pred = gb_model.predict(X_test) # Make predictions on the testing set


            # Train the Gradient Boosting model using only weather data
            X_weather_only = X[:, -len(weather_data.columns)*lag_weeks:]
            X_train_weather, y_train_weather = X_weather_only[:train_size], y[:train_size]
            X_test_weather, y_test_weather = X_weather_only[train_size:], y[train_size:]
            gb_model_weather = GradientBoostingRegressor(n_estimators=100, learning_rate=0.06, max_depth=4, subsample=0.8, random_state=42)
            gb_model_weather.fit(X_train_weather, y_train_weather)
            # Make predictions on the testing set using only weather data
            y_pred_weather = gb_model_weather.predict(X_test_weather)
            # Calculate evaluation metrics for the weather-only model
            mse_weather = mean_squared_error(y_test_weather, y_pred_weather)
            mae_weather = mean_absolute_error(y_test_weather, y_pred_weather)
            r2_weather = r2_score(y_test_weather, y_pred_weather)
            pcc_weather, _ = pearsonr(y_test_weather, y_pred_weather)
            srcc_weather, _ = spearmanr(y_test_weather, y_pred_weather)


            # Calculate evaluation metrics
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            pcc, _ = pearsonr(y_test, y_pred)
            srcc, _ = spearmanr(y_test, y_pred)
            mse_scores.append(mse)
            mae_scores.append(mae)
            r2_scores.append(r2)
            pcc_scores.append(pcc)
            srcc_scores.append(srcc)
            print(f"Topic {topic+1} - MSE: {mse:.4f}, MAE: {mae:.4f}, R-squared: {r2:.4f}, PCC: {pcc:.4f}, SRCC: {srcc:.4f}")


            # Add traces to the forecasting figure
            forecasting_fig.add_trace(go.Scatter(x=weekly_weather.index[train_size+lag_weeks:], y=y_test, mode='lines', name=f'Actual Topic {topic+1}'), row=topic+1, col=1)
            forecasting_fig.add_trace(go.Scatter(x=weekly_weather.index[train_size+lag_weeks:], y=y_pred, mode='lines', name=f'Predicted Topic {topic+1}'), row=topic+1, col=1)
            # Add predicted vs actual value graph with a diagonal line
            forecasting_fig.add_trace(go.Scatter(x=y_test, y=y_pred, mode='markers', name=f'Topic {topic+1}'), row=topic+1, col=2)
            forecasting_fig.add_shape(type='line', x0=y_test.min(), y0=y_test.min(), x1=y_test.max(), y1=y_test.max(), line=dict(color='black', dash='dash'), row=topic+1, col=2)
            # Add traces for the weather-only model
            forecasting_fig.add_trace(go.Scatter(x=weekly_weather.index[train_size+lag_weeks:], y=y_test_weather, mode='lines', name=f'Actual Topic {topic+1} (Weather Only)'), row=topic+1, col=4)
            forecasting_fig.add_trace(go.Scatter(x=weekly_weather.index[train_size+lag_weeks:], y=y_pred_weather, mode='lines', name=f'Predicted Topic {topic+1} (Weather Only)'), row=topic+1, col=4)
            forecasting_fig.add_trace(go.Scatter(x=y_test_weather, y=y_pred_weather, mode='markers', name=f'Topic {topic+1} (Weather Only)'), row=topic+1, col=5)
            forecasting_fig.add_shape(type='line', x0=y_test_weather.min(), y0=y_test_weather.min(), x1=y_test_weather.max(), y1=y_test_weather.max(), line=dict(color='black', dash='dash'), row=topic+1, col=5)
            # Add performance metrics as text for the weather-only model
            forecasting_fig.add_annotation(x=0.05, y=0.95, xref=f'x{5*topic+5} domain', yref=f'y{5*topic+5} domain',
                                        text=f"MSE: {mse_weather:.4f}<br>MAE: {mae_weather:.4f}<br>R-squared: {r2_weather:.4f}<br>PCC: {pcc_weather:.4f}<br>SRCC: {srcc_weather:.4f}",
                                        showarrow=False, align='left', font=dict(size=12), bgcolor='rgba(255, 255, 255, 0.8)')
            # Add performance metrics as text
            forecasting_fig.add_annotation(x=0.05, y=0.95, xref=f'x{5*topic+2} domain', yref=f'y{5*topic+2} domain',
                                        text=f"MSE: {mse:.4f}<br>MAE: {mae:.4f}<br>R-squared: {r2:.4f}<br>PCC: {pcc:.4f}<br>SRCC: {srcc:.4f}",
                                        showarrow=False, align='left', font=dict(size=12), bgcolor='rgba(255, 255, 255, 0.8)')
            # Update the background color and grid settings for the new graphs
            forecasting_fig.update_xaxes(showgrid=False, row=topic+1, col=4)
            forecasting_fig.update_yaxes(showgrid=False, row=topic+1, col=4)
            forecasting_fig.update_xaxes(showgrid=False, row=topic+1, col=5)
            forecasting_fig.update_yaxes(showgrid=False, row=topic+1, col=5)
            #forecasting_fig.update_layout({'plot_bgcolor': 'rgba(240, 240, 240, 0.8)'}, row=topic+1, col=4) #bgcolor doesnt work.
            #forecasting_fig.update_layout({'plot_bgcolor': 'rgba(240, 240, 240, 0.8)'}, row=topic+1, col=5) #bgcolor doesnt work.

            # Weather Impact Plot (Partial Dependence)
            if location != "Combined":
                weather_data_test = weekly_weather.iloc[train_size+lag_weeks:]
                base_weather = weather_data_test.median()

                for var in weather_vars:
                    weather_range = np.linspace(weather_data_test[var].min(), weather_data_test[var].max(), 100)
                    predictions = []
                    for val in weather_range:
                        weather_input = base_weather.copy()
                        weather_input[var] = val
                        X_input = np.concatenate((topic_strengths[-lag_weeks:], np.tile(weather_input.values, lag_weeks)))
                        X_input = X_input.reshape(1, -1)
                        prediction = gb_model.predict(X_input)
                        predictions.append(prediction[0])
                    forecasting_fig.add_trace(go.Scatter(x=weather_range, y=predictions, mode='lines', name=var), row=topic+1, col=3)

                forecasting_fig.update_xaxes(title_text="Weather Variable", row=topic+1, col=3)
                forecasting_fig.update_yaxes(title_text="Predicted Topic Strength", row=topic+1, col=3)


                # Weather Impact Plot (Partial Dependence) Seperate Figure (one graph per weather variable)
                weather_data_test = weekly_weather.iloc[train_size+lag_weeks:]
                base_weather = weather_data_test.median()

                for i, var in enumerate(weather_vars, start=1):
                    weather_range = np.linspace(weather_data_test[var].min(), weather_data_test[var].max(), 100)
                    predictions = []
                    for val in weather_range:
                        weather_input = base_weather.copy()
                        weather_input[var] = val
                        X_input = np.concatenate((topic_strengths[-lag_weeks:], np.tile(weather_input.values, lag_weeks)))
                        X_input = X_input.reshape(1, -1)
                        prediction = gb_model.predict(X_input)
                        predictions.append(prediction[0])

                    pdp_fig.add_trace(go.Scatter(x=weather_range, y=predictions, mode='lines', name=var), row=topic+1, col=i)
                    pdp_fig.update_xaxes(title_text=var, row=topic+1, col=i)
                    pdp_fig.update_yaxes(title_text="Predicted Topic Strength", row=topic+1, col=i)

        if location != "Combined":
            pdp_fig.update_layout(height=600*num_topics, width=1600, title_text=f"Partial Dependence Plots - {location}")
            pdp_fig.show()

        forecasting_fig.update_layout(title=f"Topic Strength Forecasting Performance - {location} (Testing Period)",height=600*num_topics, width=2560,showlegend=False)
        forecasting_fig.show()


        mse_scores_all.append(mse_scores)
        mae_scores_all.append(mae_scores)
        r2_scores_all.append(r2_scores)
        pcc_scores_all.append(pcc_scores)
        srcc_scores_all.append(srcc_scores)
    



    def forecast_topic_strengths_combinedalltopics(topic_strengths, weather_data, lag_weeks=4):
        X, y = [], []
        for i in range(lag_weeks, len(topic_strengths)):
            topic_lags = topic_strengths[i-lag_weeks:i].flatten()
            weather_lags = weather_data.iloc[i-lag_weeks:i].values.flatten()
            X.append(np.concatenate((topic_lags, weather_lags)))
            y.append(topic_strengths[i])
        return np.array(X), np.array(y)

    import xgboost as xgb

    # COMBINED LOCATION, ALL TOPICS - FINAL MODEL
    print(f"Processing Combined Model for All Topics")
    combined_topic_strengths = np.array([strengths for strengths in combined_topic_strengths_aligned.values()]).T
    X_combined, y_combined = forecast_topic_strengths_combinedalltopics(combined_topic_strengths, combined_weather, lag_weeks)

    # Replace missing values with the mean of the corresponding feature
    X_combined = np.where(np.isnan(X_combined), np.ma.array(X_combined, mask=np.isnan(X_combined)).mean(axis=0), X_combined)

    # Define the training and testing indices
    train_size_combined = int(0.8 * len(X_combined))
    X_train_combined, y_train_combined = X_combined[:train_size_combined], y_combined[:train_size_combined]
    X_test_combined, y_test_combined = X_combined[train_size_combined:], y_combined[train_size_combined:]

    # Create the XGBoost multi-output regressor
    xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.06, objective='reg:squarederror', subsample=0.8, colsample_bytree=0.8, random_state=42)

    xgb_model.fit(X_train_combined, y_train_combined) # Train the XGBoost model
    y_pred_combined = xgb_model.predict(X_test_combined) # Make predictions on the testing set

    # Create a new figure for the combined model
    combined_fig = make_subplots(rows=num_topics, cols=3, subplot_titles=list(chain.from_iterable([[f"Topic {i+1} - Actual vs Predicted", f"Topic {i+1} - Performance Metrics", f"Topic {i+1} - Weather Partial Dependence"] for i in range(num_topics)])))

    for topic in range(num_topics):
        y_test_topic = y_test_combined[:, topic]
        y_pred_topic = y_pred_combined[:, topic]

        # Calculate evaluation metrics
        mse_topic = mean_squared_error(y_test_topic, y_pred_topic)
        mae_topic = mean_absolute_error(y_test_topic, y_pred_topic)
        r2_topic = r2_score(y_test_topic, y_pred_topic)
        pcc_topic, _ = pearsonr(y_test_topic, y_pred_topic)
        srcc_topic, _ = spearmanr(y_test_topic, y_pred_topic)

        mse_scores_all.append(mse_topic)
        mae_scores_all.append(mae_topic)
        r2_scores_all.append(r2_topic)
        pcc_scores_all.append(pcc_topic)
        srcc_scores_all.append(srcc_topic)

        print(f"Topic {topic+1} - MSE: {mse_topic:.4f}, MAE: {mae_topic:.4f}, R-squared: {r2_topic:.4f}, PCC: {pcc_topic:.4f}, SRCC: {srcc_topic:.4f}")

        # Add traces to the combined model figure
        combined_fig.add_trace(go.Scatter(x=combined_weather.index[train_size_combined+lag_weeks:], y=y_test_topic, mode='lines', name=f'Actual Topic {topic+1}'), row=topic+1, col=1)
        combined_fig.add_trace(go.Scatter(x=combined_weather.index[train_size_combined+lag_weeks:], y=y_pred_topic, mode='lines', name=f'Predicted Topic {topic+1}'), row=topic+1, col=1)

        combined_fig.add_trace(go.Scatter(x=y_test_topic, y=y_pred_topic, mode='markers', name=f'Topic {topic+1}'), row=topic+1, col=2)
        combined_fig.add_shape(type='line', x0=y_test_topic.min(), y0=y_test_topic.min(), x1=y_test_topic.max(), y1=y_test_topic.max(), line=dict(color='black', dash='dash'), row=topic+1, col=2)

        # Add performance metrics as text
        combined_fig.add_annotation(x=0.05, y=0.95, xref=f'x{3*topic+2} domain', yref=f'y{3*topic+2} domain',
                                    text=f"MSE: {mse_topic:.4f}<br>MAE: {mae_topic:.4f}<br>R-squared: {r2_topic:.4f}<br>PCC: {pcc_topic:.4f}<br>SRCC: {srcc_topic:.4f}",
                                    showarrow=False, align='left', font=dict(size=12), bgcolor='rgba(255, 255, 255, 0.8)')

        # Weather Impact Plot (Partial Dependence)
        weather_vars = ['Chicago_PRCP', 'Chicago_TMAX', 'Chicago_SNOW', 'Chicago_TMIN', 'LA_PRCP', 'LA_TMAX', 'LA_SNOW', 'LA_TMIN']
        weather_data_test = combined_weather.iloc[train_size_combined+lag_weeks:]
        base_weather = weather_data_test.median()

        for var in weather_vars:
            weather_range = np.linspace(weather_data_test[var].min(), weather_data_test[var].max(), 100)
            predictions = []
            for val in weather_range:
                weather_input = base_weather.copy()
                weather_input[var] = val
                X_input = np.concatenate((combined_topic_strengths[-lag_weeks:].flatten(), np.tile(weather_input.values, lag_weeks)))
                X_input = X_input.reshape(1, -1)
                prediction = xgb_model.predict(X_input)
                predictions.append(prediction[0, topic])
            combined_fig.add_trace(go.Scatter(x=weather_range, y=predictions, mode='lines', name=var), row=topic+1, col=3)

        combined_fig.update_xaxes(title_text="Weather Variable", row=topic+1, col=3)
        combined_fig.update_yaxes(title_text="Predicted Topic Strength", row=topic+1, col=3)

    combined_fig.update_layout(title=f"Topic Strength Forecasting Performance - Combined Weather + Combined Topics (Testing Period)",
                            height=600*num_topics, width=1800,
                            showlegend=False)
    combined_fig.show()



    # Create a new figure for Partial Dependence Plots (Combined Model)
    combined_pdp_fig = make_subplots(rows=num_topics, cols=4, subplot_titles=[f"Topic {i+1} {var}" for i in range(num_topics) for var in ['PRCP', 'TMAX', 'SNOW', 'TMIN']])

    chicago_color = 'blue'
    la_color = 'green'

    for topic in range(num_topics):
        weather_vars = ['Chicago_PRCP', 'Chicago_TMAX', 'Chicago_SNOW', 'Chicago_TMIN', 'LA_PRCP', 'LA_TMAX', 'LA_SNOW', 'LA_TMIN']
        weather_data_test = combined_weather.iloc[train_size_combined+lag_weeks:]
        base_weather = weather_data_test.median()

        for i, var in enumerate(weather_vars, start=1):
            weather_range = np.linspace(weather_data_test[var].min(), weather_data_test[var].max(), 100)
            predictions_chicago = []
            predictions_la = []
            for val in weather_range:
                weather_input = base_weather.copy()
                weather_input[var] = val
                X_input_chicago = np.concatenate((combined_topic_strengths[-lag_weeks:].flatten(), np.tile(weather_input[['Chicago_PRCP', 'Chicago_TMAX', 'Chicago_SNOW', 'Chicago_TMIN']].values, lag_weeks), np.tile(weather_input[['LA_PRCP', 'LA_TMAX', 'LA_SNOW', 'LA_TMIN']].values, lag_weeks)))
                X_input_la = np.concatenate((combined_topic_strengths[-lag_weeks:].flatten(), np.tile(weather_input[['Chicago_PRCP', 'Chicago_TMAX', 'Chicago_SNOW', 'Chicago_TMIN']].values, lag_weeks), np.tile(weather_input[['LA_PRCP', 'LA_TMAX', 'LA_SNOW', 'LA_TMIN']].values, lag_weeks)))
                X_input_chicago = X_input_chicago.reshape(1, -1)
                X_input_la = X_input_la.reshape(1, -1)
                prediction_chicago = xgb_model.predict(X_input_chicago)
                prediction_la = xgb_model.predict(X_input_la)
                predictions_chicago.append(prediction_chicago[0, topic])
                predictions_la.append(prediction_la[0, topic])

            # Add traces
            if 'Chicago' in var:
                combined_pdp_fig.add_trace(go.Scatter(x=weather_range, y=predictions_chicago, mode='lines', line=dict(color=chicago_color), name=f'{var}'), row=topic+1, col=1+((i-1)%4))
            else:
                combined_pdp_fig.add_trace(go.Scatter(x=weather_range, y=predictions_la, mode='lines', line=dict(color=la_color), name=f'{var}'), row=topic+1, col=1+((i-1)%4))

            combined_pdp_fig.update_xaxes(title_text=var[-4:], row=topic+1, col=i)
            combined_pdp_fig.update_yaxes(title_text="Predicted Topic Strength", row=topic+1, col=i)

    combined_pdp_fig.update_layout(height=600*num_topics, width=1600, title_text=f"Partial Dependence Plots - Combined Weather + Combined Topics")
    combined_pdp_fig.show()
    




    # Create a formatted table for performance metrics
    metrics_data = []
    for location, mse_scores, mae_scores, r2_scores, pcc_scores, srcc_scores in zip(["Chicago", "LA", "Combined Weather", "Combined Weather + Combined Topics"],mse_scores_all,mae_scores_all,r2_scores_all,pcc_scores_all,srcc_scores_all):
        mse_avg = np.mean(mse_scores)
        mae_avg = np.mean(mae_scores)
        r2_avg = np.mean(r2_scores)
        pcc_avg = np.mean(pcc_scores)
        srcc_avg = np.mean(srcc_scores)
        metrics_data.append([location, mse_avg, mae_avg, r2_avg, pcc_avg, srcc_avg])

    metrics_df = pd.DataFrame(metrics_data, columns=["Model Name", "MSE", "MAE", "R-squared", "PCC", "SRCC"])
    print("\nPerformance Metrics:")
    print(metrics_df.to_string(index=False))




    from scipy.stats import t
    from scipy.stats import linregress

    # STATISTICAL TESTING
    print("Statistical tests and measures...")

    # Create lists to store confidence intervals and PDP evaluation results
    ci_data = []
    pdp_results = []

    # Perform statistical tests - we use the metric values to do statistical tests across the topics - which allows us to test if the model is performing consistently well across topics for example.
    # The final model is not used here as we only have the result for all topics (as its trained on all in one go), not one metric result per topic, so we can't confidence test that.
    confidence_level = 0.95
    for location, mse_scores, mae_scores, r2_scores, pcc_scores, srcc_scores in zip(["Chicago", "LA", "Combined Weather"], mse_scores_all, mae_scores_all, r2_scores_all, pcc_scores_all, srcc_scores_all):
        n = len(mse_scores)
        mse_mean = np.mean(mse_scores)
        mse_std = np.std(mse_scores)
        mse_ci = t.interval(confidence_level, n-1, loc=mse_mean, scale=mse_std/np.sqrt(n))
        
        mae_mean = np.mean(mae_scores)
        mae_std = np.std(mae_scores)
        mae_ci = t.interval(confidence_level, n-1, loc=mae_mean, scale=mae_std/np.sqrt(n))
        
        r2_mean = np.mean(r2_scores)
        r2_std = np.std(r2_scores)
        r2_ci = t.interval(confidence_level, n-1, loc=r2_mean, scale=r2_std/np.sqrt(n))

        pcc_mean = np.mean(pcc_scores)
        pcc_std = np.std(pcc_scores)
        pcc_ci = t.interval(confidence_level, n-1, loc=pcc_mean, scale=pcc_std/np.sqrt(n))

        srcc_mean = np.mean(srcc_scores)
        srcc_std = np.std(srcc_scores)
        srcc_ci = t.interval(confidence_level, n-1, loc=srcc_mean, scale=srcc_std/np.sqrt(n))
        
        ci_data.append([location, f"{mse_ci[0]:.4f}, {mse_ci[1]:.4f}", f"{mae_ci[0]:.4f}, {mae_ci[1]:.4f}", f"{r2_ci[0]:.4f}, {r2_ci[1]:.4f}", f"{pcc_ci[0]:.4f}, {pcc_ci[1]:.4f}", f"{srcc_ci[0]:.4f}, {srcc_ci[1]:.4f}"])

    # Evaluate significance and patterns in Partial Dependence Plots (PDPs)
    for topic in range(num_topics):
        for var in ['Chicago_PRCP', 'Chicago_TMAX', 'Chicago_SNOW', 'Chicago_TMIN', 'LA_PRCP', 'LA_TMAX', 'LA_SNOW', 'LA_TMIN']:
            weather_range = combined_weather[var].dropna().values
            if len(weather_range) <= 1:
                pdp_results.append([topic+1, var, np.nan, np.nan, np.nan, np.nan, "No data available for this weather variable."])
                continue

            weather_range = np.linspace(weather_range.min(), weather_range.max(), 100)
            predictions = []
            for val in weather_range:
                weather_input = combined_weather.median().copy()
                weather_input[var] = val
                X_input = np.concatenate((combined_topic_strengths[-lag_weeks:].flatten(), np.tile(weather_input.values, lag_weeks)))
                X_input = X_input.reshape(1, -1)
                prediction = xgb_model.predict(X_input)
                predictions.append(prediction[0, topic])

            try:
                slope, intercept, r_value, p_value, std_err = linregress(weather_range, predictions)
            except ValueError:
                pdp_results.append([topic+1, var, np.nan, np.nan, np.nan, np.nan, "Error: Linear regression failed due to constant input values."])
                continue
            
            if abs(slope) > 0.001 and p_value < 0.05:
                interpretation = f"Significant effect observed. A change in {var} is associated with a substantial change in topic strength."
            elif p_value >= 0.05:
                interpretation = f"No significant effect observed."
            else:
                interpretation = f"While the effect is statistically significant, the magnitude of the change in topic strength is negligible."

            pdp_results.append([topic+1, var, slope, intercept, r_value, p_value, interpretation])

    print("Confidence Intervals:")
    ci_df = pd.DataFrame(ci_data, columns=['Model Name', 'MSE CI', 'MAE CI', 'R-squared CI', 'PCC CI', 'SRCC CI'])
    print(ci_df.to_string(index=False))

    print("\nPartial Dependence Plot (PDP) Evaluation:")
    pdp_df = pd.DataFrame(pdp_results, columns=['Topic', 'Weather Variable', 'Slope', 'Intercept', 'R-value', 'p-value', 'Interpretation'])
    print(pdp_df.to_string(index=False))
    

if __name__ == '__main__':
    main()